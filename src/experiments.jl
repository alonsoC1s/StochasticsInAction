using JuMP, GLPK
using Distributions
using LinearAlgebra
using Plots

struct MDP
    Î³  # discount factor
    ð’®  # state space
    ð’œ  # action space
    T  # transition function
    R  # reward function
    TR # sample transition and reward
end

MDP(Î³, ð’®, ð’œ, T, R) = MDP(Î³, ð’®, ð’œ, T, R, nothing)

function MDP(T::Array{Float64,3}, R::Array{Float64,2}, Î³::Float64)
    MDP(Î³, 1:size(R, 1), 1:size(R, 2), (s, a, sâ€²) -> T[s, a, sâ€²], (s, a) -> R[s, a], nothing)
end

function MDP(T::Array{Float64,3}, R, Î³::Float64)
    MDP(Î³, 1:size(T, 1), 1:size(T, 2), (s, a, sâ€²) -> T[s, a, sâ€²], R, nothing)
end

struct MDPInitialStateDistribution{MDP}
    mdp::MDP
end

Base.rand(S::MDPInitialStateDistribution) = generate_start_state(S.mdp)

struct DiscreteMDP
    # TODO: Use sparse matrices?
    T::Array{Float64,3} # T(s,a,sâ€²)
    R::Array{Float64,2} # R(s,a) = âˆ‘_s' R(s,a,s')*T(s,a,sâ€²)
    Î³::Float64
end

n_states(mdp::DiscreteMDP) = size(mdp.T, 1)
n_actions(mdp::DiscreteMDP) = size(mdp.T, 2)
discount(mdp::DiscreteMDP) = mdp.Î³
ordered_states(mdp::DiscreteMDP) = collect(1:n_states(mdp))
ordered_actions(mdp::DiscreteMDP) = collect(1:n_actions(mdp))
state_index(mdp::DiscreteMDP, s::Int) = s

function transition(mdp::DiscreteMDP, s::Int, a::Int)
    return Categorical(mdp.T[s, a, :])
end
function generate_s(mdp::DiscreteMDP, s::Int, a::Int)
    sâ€² = 1
    r = rand() - mdp.T[s, a, sâ€²]
    while r > 0.0 && sâ€² < size(mdp.T, 3)
        sâ€² += 1
        r -= mdp.T[s, a, sâ€²]
    end
    return sâ€²
end

reward(mdp::DiscreteMDP, s::Int, a::Int) = mdp.R[s, a]

function MDP(mdp::DiscreteMDP; Î³::Float64=discount(mdp))
    return MDP(
        Î³,
        ordered_states(mdp),
        ordered_actions(mdp),
        (s, a, sâ€²=nothing) -> begin
            Sâ€² = transition(mdp, s, a)
            if sâ€² == nothing
                return Sâ€²
            end
            return pdf(Sâ€², sâ€²)
        end,
        (s, a) -> reward(mdp, s, a),
        (s, a) -> begin
            sâ€² = rand(transition(mdp, s, a))
            r = reward(mdp, s, a)
            return (sâ€², r)
        end
    )
end

function hex_neighbors(hex::Tuple{Int,Int})
    i, j = hex
    [(i + 1, j), (i, j + 1), (i - 1, j + 1), (i - 1, j), (i, j - 1), (i + 1, j - 1)]
end

struct HexWorldMDP
    # Problem has |hexes| + 1 states, where last state is consuming.
    hexes::Vector{Tuple{Int,Int}}

    # The exact same problem as a DiscreteMDP
    mdp::DiscreteMDP

    # The special hex rewards used to construct the MDP
    special_hex_rewards::Dict{Tuple{Int,Int},Float64}

    function HexWorldMDP(
        hexes::Vector{Tuple{Int,Int}},
        r_bump_border::Float64,
        p_intended::Float64,
        special_hex_rewards::Dict{Tuple{Int,Int},Float64},
        Î³::Float64,
    )

        nS = length(hexes) + 1 # Hexes plus one terminal state
        nA = 6 # Six directions. 1 is east, 2 is north east, 3 is north west, etc.
        # As enumerated in hex_neighbors.

        s_absorbing = nS

        T = zeros(Float64, nS, nA, nS)
        R = zeros(Float64, nS, nA)

        p_veer = (1.0 - p_intended) / 2 # Odds of veering left or right.

        for s in 1:length(hexes)
            hex = hexes[s]
            if !haskey(special_hex_rewards, hex)
                # Action taken from a normal tile
                neighbors = hex_neighbors(hex)
                for (a, neigh) in enumerate(neighbors)
                    # Indended transition.
                    sâ€² = findfirst(h -> h == neigh, hexes)
                    if sâ€² == nothing
                        # Off the map!
                        sâ€² = s
                        R[s, a] += r_bump_border * p_intended
                    end
                    T[s, a, sâ€²] += p_intended

                    # Unintended veer left.
                    a_left = mod1(a + 1, nA)
                    neigh_left = neighbors[a_left]
                    sâ€² = findfirst(h -> h == neigh_left, hexes)
                    if sâ€² == nothing
                        # Off the map!
                        sâ€² = s
                        R[s, a] += r_bump_border * p_veer
                    end
                    T[s, a, sâ€²] += p_veer

                    # Unintended veer right.
                    a_right = mod1(a - 1, nA)
                    neigh_right = neighbors[a_right]
                    sâ€² = findfirst(h -> h == neigh_right, hexes)
                    if sâ€² == nothing
                        # Off the map!
                        sâ€² = s
                        R[s, a] += r_bump_border * p_veer
                    end
                    T[s, a, sâ€²] += p_veer
                end
            else
                # Action taken from an absorbing hex
                # In absorbing hex, your action automatically takes you to the absorbing state and you get the reward.
                for a in 1:nA
                    T[s, a, s_absorbing] = 1.0
                    R[s, a] += special_hex_rewards[hex]
                end
            end
        end

        # Absorbing state stays where it is and gets no reward.
        for a in 1:nA
            T[s_absorbing, a, s_absorbing] = 1.0
        end

        mdp = DiscreteMDP(T, R, Î³)

        return new(hexes, mdp, special_hex_rewards)
    end
end

const HexWorldRBumpBorder = -1.0 # Reward for falling off hex map
const HexWorldPIntended = 0.7 # Probability of going intended direction
const HexWorldDiscountFactor = 0.9

function HexWorld()
    HexWorld = HexWorldMDP(
        [(0, 0), (1, 0), (2, 0), (3, 0), (0, 1), (1, 1), (2, 1), (-1, 2),
            (0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (5, 2), (6, 2), (7, 2),
            (8, 2), (4, 1), (5, 0), (6, 0), (7, 0), (7, 1), (8, 1), (9, 0)],
        HexWorldRBumpBorder,
        HexWorldPIntended,
        Dict{Tuple{Int,Int},Float64}(
            (0, 1) => 5.0, # left side reward
            (2, 0) => -10.0, # left side hazard
            (9, 0) => 10.0, # right side reward
        ),
        HexWorldDiscountFactor
    )
    return HexWorld
end

# function StraightLineHexWorld()
#     StraightLineHexWorld = HexWorldMDP(
#         [(0, 0), (1, 0), (2, 0), (3, 0), (0, 1), (1, 1), (2, 1), (-1, 2),
#             (0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (5, 2), (6, 2), (7, 2),
#             (8, 2), (4, 1), (5, 0), (6, 0), (7, 0), (7, 1), (8, 1), (9, 0)],
#         HexWorldRBumpBorder,
#         HexWorldPIntended,
#         Dict{Tuple{Int,Int},Float64}(
#             (0, 1) => 5.0, # left side reward
#             (2, 0) => -10.0, # left side hazard
#             (9, 0) => 10.0, # right side reward
#         ),
#         HexWorldDiscountFactor
#     )
#     return StraightLineHexWorld
# end
function StraightLineHexWorld()
    StraightLineHexWorld = HexWorldMDP(
        [(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0)],
        HexWorldRBumpBorder,
        HexWorldPIntended,
        Dict{Tuple{Int,Int},Float64}(
            (7, 0) => 10.0, # right side reward
        ),
        HexWorldDiscountFactor
    )
    return StraightLineHexWorld
end

n_states(mdp::HexWorldMDP) = n_states(mdp.mdp)
n_actions(mdp::HexWorldMDP) = n_actions(mdp.mdp)
discount(mdp::HexWorldMDP) = discount(mdp.mdp)
ordered_states(mdp::HexWorldMDP) = ordered_states(mdp.mdp)
ordered_actions(mdp::HexWorldMDP) = ordered_actions(mdp.mdp)
state_index(mdp::HexWorldMDP, s::Int) = s

transition(mdp::HexWorldMDP, s::Int, a::Int) = transition(mdp.mdp, s, a)
generate_s(mdp::HexWorldMDP, s::Int, a::Int) = generate_s(mdp.mdp, s, a)
reward(mdp::HexWorldMDP, s::Int, a::Int) = reward(mdp.mdp, s, a)
generate_sr(mdp::HexWorldMDP, s::Int, a::Int) = (generate_s(mdp, s, a), reward(mdp, s, a))

generate_start_state(mdp::HexWorldMDP) = rand(1:(n_states(mdp)-1)) # non-terminal state

function hex_distance(a::Tuple{Int,Int}, b::Tuple{Int,Int})
    az = -a[1] - a[2]
    bz = -b[1] - b[2]
    return max(abs(a[1] - b[1]), abs(a[2] - b[2]), abs(az - bz))
end

function DiscreteMDP(mdp::HexWorldMDP)
    return mdp.mdp
end
function MDP(mdp::HexWorldMDP)
    return MDP(mdp.mdp)
end

struct ValueFunctionPolicy
    ð’« # problem
    U # utility function
end

function lookahead(ð’«::MDP, U, s, a)
    ð’®, T, R, Î³ = ð’«.ð’®, ð’«.T, ð’«.R, ð’«.Î³
    return R(s,a) + Î³*sum(T(s,a,sâ€²)*U(sâ€²) for sâ€² in ð’®)
end
function lookahead(ð’«::MDP, U::Vector, s, a)
    ð’®, T, R, Î³ = ð’«.ð’®, ð’«.T, ð’«.R, ð’«.Î³
    return R(s,a) + Î³*sum(T(s,a,sâ€²)*U[i] for (i,sâ€²) in enumerate(ð’®))
end

function greedy(ð’«::MDP, U, s)
    u, a = findmax(a -> lookahead(ð’«, U, s, a), ð’«.ð’œ)
    return (a=a, u=u)
end

(Ï€::ValueFunctionPolicy)(s) = greedy(Ï€.ð’«, Ï€.U, s).a

struct LinearProgramFormulation end

function tensorform(ð’«::MDP)
    ð’®, ð’œ, R, T = ð’«.ð’®, ð’«.ð’œ, ð’«.R, ð’«.T
    ð’®â€² = eachindex(ð’®)
    ð’œâ€² = eachindex(ð’œ)
    Râ€² = [R(s, a) for s in ð’®, a in ð’œ]
    Tâ€² = [T(s, a, sâ€²) for s in ð’®, a in ð’œ, sâ€² in ð’®]
    return ð’®â€², ð’œâ€², Râ€², Tâ€²
end

solve(ð’«::MDP) = solve(LinearProgramFormulation(), ð’«)

function solve(M::LinearProgramFormulation, ð’«::MDP)
    ð’®, ð’œ, R, T = tensorform(ð’«)
    model = Model(GLPK.Optimizer)
    @variable(model, U[ð’®])
    @objective(model, Min, sum(U))
    @constraint(model, [s = ð’®, a = ð’œ], U[s] â‰¥ R[s, a] + ð’«.Î³ * T[s, a, :] â‹… U)
    optimize!(model)
    return U, ValueFunctionPolicy(ð’«, value.(U))
end

slhw = StraightLineHexWorld()
v_star, Ï€olicy = solve(LinearProgramFormulation(), slhw |> MDP)

hexes = reinterpret(reshape, Int, slhw.hexes) |> transpose |> Matrix

scatter(eachcol(hexes)...,
    marker = :hex,
    markersize = 10,
    zcolor = value.(v_star)
)

